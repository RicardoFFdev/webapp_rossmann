{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import warnings\n",
    "import inflection\n",
    "import numpy               as np\n",
    "import pandas              as pd\n",
    "import seaborn             as sns\n",
    "import xgboost             as xgb\n",
    "import matplotlib.pyplot   as plt\n",
    "\n",
    "from scipy                 import stats\n",
    "from boruta                import BorutaPy\n",
    "from tabulate              import tabulate\n",
    "from dython.nominal        import associations\n",
    "from IPython.display       import Image\n",
    "from sklearn.metrics       import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from matplotlib.gridspec   import GridSpec\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar as médias das previsões do modelo de ML\n",
    "def ml_error (model_name, y, yhat):\n",
    "    mae = mean_absolute_error(y, yhat)\n",
    "    mape = mean_absolute_percentage_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "\n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                         'MAE': mae,\n",
    "                         'MAPE': mape,\n",
    "                         'RMSE': rmse}, index=[0])\n",
    "\n",
    "# Ajuste de tamanho para os gráficos criados\n",
    "sns.set(rc={'figure.figsize':(12, 10)})\n",
    "\n",
    "# Atalho para o uso das funções das bibliotecas de ajuste estatístico\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "# Cramer's V -> fórmula para se medir a correlação de variáveis categóricas simetricamente\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "\n",
    "# Theil's U -> fórmula para se medir a correlação de variáveis categóricas simetricamente\n",
    "def theils_u(x, y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x\n",
    "    \n",
    "# Função de aplicação do Cross Validation\n",
    "def cross_validation(x_training, kfold, model_name, model, verbose=False):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    for k in reversed(range(1, kfold+1)):\n",
    "        if verbose:\n",
    "            print('KFold Number: {}'.format(k))\n",
    "\n",
    "        # start and end date for validation\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta(days=k*6*7)            \n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta(days=(k-1)*6*7)\n",
    "        \n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "        \n",
    "        # training and validation dataset\n",
    "        #training\n",
    "        xtraining = training.drop(['date', 'sales'], axis=1)\n",
    "        ytraining = training['sales']\n",
    "        \n",
    "        # validation\n",
    "        xvalidation = validation.drop(['date', 'sales'], axis=1)\n",
    "        yvalidation = validation['sales']\n",
    "        \n",
    "        # model\n",
    "        m = model.fit(xtraining, ytraining)\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict(xvalidation)\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error(model_name, np.expm1(yvalidation), np.expm1(yhat))\n",
    "\n",
    "        # store performance of each kfold interation\n",
    "        mae_list.append(m_result['MAE'])\n",
    "        mape_list.append(m_result['MAPE'])\n",
    "        rmse_list.append(m_result['RMSE'])\n",
    "\n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                         'MAE CV': round(np.mean(mae_list), 2).astype(str) + ' +/- ' + round(np.std(mae_list), 2).astype(str),\n",
    "                         'MAPE CV': round(np.mean(mape_list), 2).astype(str) + ' +/- ' + round(np.std(mape_list), 2).astype(str),\n",
    "                         'RMSE CV': round(np.mean(rmse_list), 2).astype(str) + ' +/- ' + round(np.std(rmse_list), 2).astype(str)}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv('csv_files/train.csv', low_memory=False)\n",
    "df_store_raw = pd.read_csv('csv_files/store.csv', low_memory=False)\n",
    "\n",
    "# Merging\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "       'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "       'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "       'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "cols_new = list(map(snakecase, cols_old))\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#competition_distance -> filling the dataset with a high distance value\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "#competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "#competition_open_since_year\n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "#promo2_since_week\n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "#promo2_since_year\n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "#promo_interval\n",
    "month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Descriptive Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(include=['int64', 'float64'])\n",
    "cat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).T\n",
    "d2 = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).T\n",
    "d3 = pd.concat([d1, d2]).T.reset_index()\n",
    "d3.columns = ['attributes', 'skew', 'kurtosis']\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df1['competition_distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(rc={'figure.figsize':(15, 10)})\n",
    "aux1 = df1[(df1['state_holiday'] != 0) & (df1['sales'] > 0 )]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxenplot(x='state_holiday' , y='sales' , data=aux1 )\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxenplot(x='store_type' , y='sales' , data=aux1 )\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxenplot(x='assortment' , y='sales' , data=aux1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/mind_map_hypthesis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Hypothesis\n",
    "\n",
    "1. Lojas com maior sortimentos vendem mais?\n",
    "2. Lojas com competidores mais próximos vendem menos?\n",
    "3. Lojas com competidores há mais tempo vendem mais?\n",
    "4. Lojas com promoções ativas por mais tempo vendem mais?\n",
    "5. Lojas com promoções consecutivas vendem mais?\n",
    "6. Lojas abertas durante o feriado do Natal vendem mais?\n",
    "7. As lojas estão vendendo mais ao longo dos anos?\n",
    "8. Lojas vendem mais nos finais de semana?\n",
    "10. Lojas vendem mais depois do 10º dia do mês?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "\n",
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year= x['competition_open_since_year'], month= x['competition_open_since_month'], day=1), axis=1)    \n",
    "df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w' ) - datetime.timedelta(days= 7))\n",
    "df2['promo_time_week'] = ((df2['date'] - df2['promo_since']) / 7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# assortment\n",
    "df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Row Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[(['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Column Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 EDA (Exploration Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df4['sales'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Numerical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes.hist(bins=25, figsize=(25, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['state_holiday'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(18, 12)\n",
    "\n",
    "# State holiday\n",
    "plt.subplot(3, 2, 1)\n",
    "aux = df4[df4['state_holiday'] != 'regular_day']\n",
    "sns.countplot(x='state_holiday', data=aux)\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', fill=True)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', fill=True)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', fill=True)\n",
    "\n",
    "# Store Type\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.countplot(data=df4, x= 'store_type')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'a']['sales'], label='a', fill=True)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'b']['sales'], label='b', fill=True)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'c']['sales'], label='c', fill=True)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'd']['sales'], label='d', fill=True)\n",
    "\n",
    "# Assortment\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.countplot(data=df4, x= 'assortment')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'basic']['sales'], label='basic', fill=True)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extra']['sales'], label='extra', fill=True)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extended']['sales'], label='extended', fill=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Bivariate Analysis (Hypothsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Lojas com maior sortimentos vendem mais?\n",
    "\n",
    "-> A hipótese é FALSA. As lojas que possuem maior sortimento VENDEM MENOS no geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15, 5);\n",
    "\n",
    "aux1 = df4[['assortment', 'sales']].groupby('assortment').sum().reset_index()\n",
    "sns.barplot(x='assortment', y='sales', data=aux1);\n",
    "\n",
    "aux2 = df4[['year_week', 'assortment', 'sales']].groupby(['year_week', 'assortment']).sum().reset_index()\n",
    "aux2.pivot(index='year_week', columns='assortment', values='sales').plot();\n",
    "\n",
    "aux3 = aux2[aux2['assortment'] == 'extra']\n",
    "aux3.pivot(index='year_week', columns='assortment', values='sales').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Lojas com competidores mais próximos vendem menos?\n",
    "-> A hipótese é FALSA. Lojas com COMPETIDORES MAIS PRÓXIMOS VENDEM MAIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(20, 7);\n",
    "\n",
    "aux1 = df4[['competition_distance', 'sales']].groupby('competition_distance').sum().reset_index()\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(x='competition_distance', y='sales', data=aux1);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "bins = list(np.arange(0, 20000, 1000))\n",
    "aux1['competition_distance'] = pd.cut(aux1['competition_distance'], bins=bins)\n",
    "aux2 = aux1[['competition_distance', 'sales']].groupby('competition_distance').sum().reset_index()\n",
    "sns.barplot(x='competition_distance', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Lojas com competidores há mais tempo vendem mais?\n",
    "->  A hipótese é FALSA. Lojas com competidores há mais tempo VENDEM MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(20, 10);\n",
    "\n",
    "# linha, coluna, posição\n",
    "plt.subplot(2, 1, 1)\n",
    "aux1 = df4[['competition_time_month', 'sales']].groupby('competition_time_month').sum().reset_index()\n",
    "aux2 = aux1[(aux1['competition_time_month'] < 120) & (aux1['competition_time_month'] != 0)]\n",
    "sns.barplot(x='competition_time_month', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.regplot(x='competition_time_month', y='sales', data=aux2);\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Lojas com promoções ativas por mais tempo vendem mais?\n",
    "-> A hipótese é FALSA. Depois de um certo período de promoção as lojas passam a vender MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(25, 10);\n",
    "aux1 = df4[['promo_time_week', 'sales']].groupby('promo_time_week').sum().reset_index()\n",
    "grid = GridSpec(2,3)\n",
    "\n",
    "# promo extendido\n",
    "aux2 = aux1[aux1['promo_time_week'] > 0]\n",
    "plt.subplot(grid[0,0])\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "plt.subplot(grid[0,1])\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux2);\n",
    "\n",
    "# promo regular\n",
    "aux3 = aux1[aux1['promo_time_week'] < 0]\n",
    "plt.subplot(grid[1,0])\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux3);\n",
    "plt.xticks(rotation=90);\n",
    "plt.subplot(grid[1,1])\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux3);\n",
    "\n",
    "# correlação\n",
    "plt.subplot(grid[2:])\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Lojas com promoções consecutivas vendem mais?\n",
    "-> A hipótese é FALSA. As lojas com promoções consecutivas vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize']=(16, 8);\n",
    "aux1 = df4[(df4['promo'] == 1) & (df4['promo2'] == 1)][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "#ax = aux1.plot()\n",
    "aux2 = df4[(df4['promo'] == 1) & (df4['promo2'] == 0)][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "#aux2.plot(ax=ax)\n",
    "#ax.legend(labels=['Tradicional e Extendida', 'Extendida']);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=\"year_week\", y=\"sales\", color='blue', data=aux1, ax=ax);\n",
    "sns.lineplot(x=\"year_week\", y=\"sales\", color='red', data=aux2, ax=ax);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Lojas abertas durante o feriado do Natal vendem mais?\n",
    "-> A hipótese é FALSA. As lojas abertas durante o feriado do Natal vendem MENOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "aux1 = df4[df4['state_holiday'] != 'regular_day'][['state_holiday', 'sales']].groupby('state_holiday').sum().reset_index() \n",
    "sns.barplot(x='state_holiday', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "aux2 = df4[df4['state_holiday'] != 'regular_day'][['state_holiday', 'year', 'sales']].groupby(['state_holiday', 'year']).sum().reset_index() \n",
    "sns.barplot(x='year', y='sales', hue='state_holiday', data=aux2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. As lojas estão vendendo mais ao longo dos anos?\n",
    "-> A hipótese é FALSA. As vendas estão diminuindo ao longo dos anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "aux1 = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.barplot(x='year', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "aux1 = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.regplot(x='year', y='sales', data=aux1)\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "aux1 = df4[['year', 'sales']].groupby('year').sum().reset_index()\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Lojas vendem menos nos finais de semana?\n",
    "-> A hipótese é VERDADEIRA. As lojas vendem MENOS nos finais de semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSpec(1,3)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "aux1 = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.barplot(x='day_of_week', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "aux1 = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.regplot(x='day_of_week', y='sales', data=aux1)\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(grid[0,2])\n",
    "aux1 = df4[['day_of_week', 'sales']].groupby('day_of_week').sum().reset_index()\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Lojas vendem mais depois do 10º dia do mês?\n",
    "-> A hipótese é VERDADEIRA. As lojas vendem MAIS depois do 10º do mês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSpec(2,3)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "aux1 = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.barplot(x='day', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "aux1 = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.regplot(x='day', y='sales', data=aux1)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(grid[0,2])\n",
    "aux1 = df4[['day', 'sales']].groupby('day').sum().reset_index()\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True)\n",
    "\n",
    "aux1['before_after'] = aux1['day'].apply(lambda x: 'before_day_10' if x <= 10 else 'after_day_10')\n",
    "aux2 = aux1[['before_after', 'sales']].groupby('before_after').sum().reset_index()\n",
    "\n",
    "plt.subplot(grid[1,:])\n",
    "sns.barplot(x='before_after', y='sales', data=aux2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resumo da checagem de Hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab =[['Hipoteses', 'Conclusao', 'Relevancia'],\n",
    "['H1', 'Falsa', 'Baixa'],\n",
    "['H2', 'Falsa', 'Media'],\n",
    "['H3', 'Falsa', 'Media'],\n",
    "['H4', 'Falsa', 'Baixa'],\n",
    "['H5', 'Falsa', 'Baixa'],\n",
    "['H6', 'Falsa', 'Media'],\n",
    "['H7', 'Falsa', 'Alta'],\n",
    "['H8', 'Verdadeira', 'Alta'],\n",
    "['H9', 'Verdadeira', 'Alta']]\n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr(method='pearson')\n",
    "sns.heatmap(correlation, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos valores categóricos\n",
    "aux = df4.select_dtypes(include='object')\n",
    "\n",
    "# Aplicação da correlação categórica de Cramer's V\n",
    "aux1 = cramers_v(aux['state_holiday'], aux['state_holiday'])\n",
    "aux2 = cramers_v(aux['state_holiday'], aux['store_type'])\n",
    "aux3 = cramers_v(aux['state_holiday'], aux['assortment'])\n",
    "\n",
    "aux4 = cramers_v(aux['store_type'], aux['state_holiday'])\n",
    "aux5 = cramers_v(aux['store_type'], aux['store_type'])\n",
    "aux6 = cramers_v(aux['store_type'], aux['assortment'])\n",
    "\n",
    "aux7 = cramers_v(aux['assortment'], aux['state_holiday'])\n",
    "aux8 = cramers_v(aux['assortment'], aux['store_type'])\n",
    "aux9 = cramers_v(aux['assortment'], aux['assortment'])\n",
    "\n",
    "aux10 = pd.DataFrame({'state_holiday': [aux1, aux2, aux3],\n",
    "                      'store_type': [aux4, aux5, aux6], \n",
    "                      'assortment': [aux7, aux8, aux9]})\n",
    "\n",
    "aux10 = aux10.set_index(aux10.columns)\n",
    "\n",
    "sns.heatmap(aux10, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação da correlação categórica de Theil's U (uncertainty coefficient)\n",
    "associations(aux10, nom_nom_assoc='theil', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Data Preparation for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não há atributos para normalizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df5.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# ajuste da coluna 'competition_distance' -> RobustScaler\n",
    "df5['competition_distance'] = rs.fit_transform(df5[['competition_distance']].values)\n",
    "pickle.dump(rs, open('parameter/competition_distance_scaler.pkl', 'wb'))\n",
    "\n",
    "# ajuste da coluna 'competition_time_month' -> RobustScaler\n",
    "df5['competition_time_month'] = rs.fit_transform(df5[['competition_time_month']].values)\n",
    "pickle.dump(rs, open('parameter/competition_time_month_scaler.pkl', 'wb'))\n",
    "\n",
    "# ajuste da coluna 'promo_time_week' -> MinMaxScaler\n",
    "df5['promo_time_week'] = mms.fit_transform(df5[['promo_time_week']].values)\n",
    "pickle.dump(mms, open('parameter/promo_time_week_scaler.pkl', 'wb'))\n",
    "\n",
    "# ajuste da coluna 'year' -> MinMaxScaler\n",
    "df5['year'] = mms.fit_transform(df5[['year']].values)\n",
    "pickle.dump(mms, open('parameter/year_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformação dos conteúdos 'object' para 'int64' -> One Hot Encoding\n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "# transformção dos conteúdos 'object' para 'int64' -> Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'])\n",
    "pickle.dump(le, open('parameter/store_type_scaler.pkl', 'wb'))\n",
    "\n",
    "# transformação dos conteúdos 'object' para 'int64' -> Ordinal Encoding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map(assortment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2 Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversão do valores da coluna sales de valores para o logaritmo -> que é o valor oposto da exponenciação, ex: 10**3 = 1000 == log10 = 3  \n",
    "df5['sales'] = np.log1p(df5['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.3 Nature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajuste dos períodos de tempo dos dias do mês, dias da semana, semanas e meses para valores cíclicos utilizando as fórmulas de seno e coseno\n",
    "# colunas dos dias da semana\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * (2. * np.pi / 7)))\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * (2. * np.pi / 7)))\n",
    "\n",
    "# dias do mês\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * (2. * np.pi / 30)))\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * (2. * np.pi / 30)))\n",
    "\n",
    "# semanas\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * (2. * np.pi / 52)))\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * (2. * np.pi / 52)))\n",
    "\n",
    "# meses\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * (2. * np.pi / 12)))\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * (2. * np.pi / 12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Spliting dateframe for test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "df6 = df6.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta(days=6*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "#test dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print('Training Min Date: {}'.format(X_train['date'].min()))\n",
    "print('Training Min Date: {}'.format(X_train['date'].max()))\n",
    "\n",
    "print('\\nTraining Min Date: {}'.format(X_test['date'].min()))\n",
    "print('Training Min Date: {}'.format(X_test['date'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test dataset for Boruta\n",
    "x_train_n = X_train.drop(['date', 'sales'], axis=1).values\n",
    "y_train_n = y_train.values.ravel()\n",
    "\n",
    "# # defining RandomForestRegressor\n",
    "# rf = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42).fit(x_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1 Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# # best features\n",
    "# x_train_fs = X_train.drop(['date', 'sales'], axis=1)\n",
    "# cols_selected_boruta = x_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "# # not selected for boruta\n",
    "# cols_not_selected_boruta = np.setdiff1d(x_train_fs.columns, cols_selected_boruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3 Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foi adicionada manualmente a coluna 'month_sin' que havia sido excluída pelo Boruta\n",
    "cols_selected_boruta = ['store',\n",
    "                        'promo',\n",
    "                        'store_type',\n",
    "                        'assortment',\n",
    "                        'competition_distance',\n",
    "                        'competition_open_since_month',\n",
    "                        'competition_open_since_year',\n",
    "                        'promo2',\n",
    "                        'promo2_since_week',\n",
    "                        'promo2_since_year',\n",
    "                        'competition_time_month',\n",
    "                        'promo_time_week',\n",
    "                        'day_of_week_sin',\n",
    "                        'day_of_week_cos',\n",
    "                        'day_sin',\n",
    "                        'day_cos',\n",
    "                        'month_sin',\n",
    "                        'month_cos',\n",
    "                        'week_of_year_sin',\n",
    "                        'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "### Desativados para poder apontar os resultados dos algoritmos\n",
    "# final features\n",
    "#cols_selected_boruta.extend(feat_to_add)\n",
    "\n",
    "# Definindo as colunas para poder realizar a Cross Validation\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend(feat_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos conjunto de dados para executar os algoritmos de ML\n",
    "x_train = X_train[cols_selected_boruta]\n",
    "x_training = X_train[cols_selected_boruta_full]\n",
    "x_test = X_test[cols_selected_boruta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1 Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1['sales'] = y_test.copy()\n",
    "\n",
    "# prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename(columns={'sales': 'predictions'})\n",
    "aux1 = pd.merge(aux1, aux2, how='left', on='store')\n",
    "yhat_baseline = aux1['predictions']\n",
    "\n",
    "# performance\n",
    "baseline_result = ml_error('Average Model', np.expm1(y_test), np.expm1(yhat_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2 Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lr = lr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error('Linear Regression', np.expm1(y_test), np.expm1(yhat_lr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2.1 Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation(x_training, 5, 'Linear Regression', lr, verbose=False )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3 Linear Regression Regularized Model - Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lrr = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lrr_result = ml_error('Linear Regression - Lasso', np.expm1(y_test), np.expm1(yhat_lrr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3.1 Lasso - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation(x_training, 5, 'Linear Regression Regularized', lrr, verbose=False )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_rf = rf.predict(x_test)\n",
    "\n",
    "# performance\n",
    "rf_result = ml_error('Random Forest Regressor', np.expm1(y_test), np.expm1(yhat_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.4.1 Random Forest Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result_cv = cross_validation(x_training, 5, 'Random Forest Regressor', rf, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                             n_estimators=128,\n",
    "                             eta=0.3,\n",
    "                             max_depth=6,\n",
    "                             subsample=1).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_xgb = model_xgb.predict(x_test)\n",
    "\n",
    "# performance\n",
    "xgb_result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.5.1 XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result_cv = cross_validation(x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.6 Comparing Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result = pd.concat([baseline_result, lr_result, lrr_result, rf_result, xgb_result])\n",
    "modelling_result.sort_values('RMSE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.6.1 Comparing Model's Performance - Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result_cv = pd.concat([lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv])\n",
    "modelling_result_cv.sort_values('RMSE CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.0 Hyperparameter Fine Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.1 Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'n_estimators': [128, 256, 512],\n",
    "         'eta': [0.01, 0.03, 0.3],\n",
    "         'max_depth': [3, 5, 6, 9],\n",
    "         'subsample': [0.1, 0.5, 0.7, 1],\n",
    "         'colsample_bytree': [0.3, 0.7, 0.9, 1]}\n",
    "\n",
    "iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_result = pd.DataFrame()\n",
    "\n",
    "# for x in range (iterations):\n",
    "#     # choose values for parameters randomly\n",
    "#     hp = {y: random.sample(v, 1)[0] for y, v in param.items()}\n",
    "#     print(hp)\n",
    "\n",
    "#     # model \n",
    "#     model_xgb = xgb.XGBRegressor(objective= 'reg:squarederror',\n",
    "#                                  n_estimators= hp['n_estimators'],\n",
    "#                                  eta=hp['eta'],\n",
    "#                                  max_depth=hp['max_depth'],\n",
    "#                                  subsample=hp['subsample'],\n",
    "#                                  colsample_bytree=hp['colsample_bytree'])\n",
    "\n",
    "#     # performance\n",
    "#     result = cross_validation(x_training, 3, 'XGBoost Regressor', model_xgb, verbose=False)\n",
    "#     final_result = pd.concat([final_result, result])\n",
    "\n",
    "# final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/Random Search.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tuned = {'n_estimators': 3000,\n",
    "               'eta':0.03 ,\n",
    "               'max_depth': 5,\n",
    "               'subsample': 0.7,\n",
    "               'colsample_bytree': 0.7,\n",
    "               'min_child_weight': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb_tuned = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                   n_estimators=param_tuned['n_estimators'],\n",
    "                                   eta=param_tuned['eta'],\n",
    "                                   max_depth=param_tuned['max_depth'],\n",
    "                                   subsample=param_tuned['subsample'],\n",
    "                                   colsample_bytree=param_tuned['colsample_bytree'],\n",
    "                                   min_child_weight=param_tuned['min_child_weight']).fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict(x_test)\n",
    "\n",
    "# performance\n",
    "xgb_result_tuned = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(xgb_result_tuned, open('model_dump/model_rossman.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_dump/model_rossman.pkl', 'rb') as data_dumped:\n",
    "#     pickle_dumped = pickle.load(data_dumped)\n",
    "\n",
    "# pickle_dumped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.0 Errors Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montando a base de dados para a nova etapa\n",
    "df9 = X_test[cols_selected_boruta_full]\n",
    "\n",
    "# rescale\n",
    "df9['sales'] = np.expm1(df9['sales'])\n",
    "df9['predictions'] = np.expm1(yhat_xgb_tuned)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.1 Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soma das predições\n",
    "df91 = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "\n",
    "# Erros: MAE e MAPE\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_error(x['sales'], x['predictions'])).reset_index().rename(columns={0: 'MAE'})\n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error(x['sales'], x['predictions'])).reset_index().rename(columns={0: 'MAPE'})\n",
    "\n",
    "# Agrupando\n",
    "df9_aux3 = pd.merge(df9_aux1, df9_aux2, how='inner', on='store')\n",
    "df92 = pd.merge(df91, df9_aux3, how='inner', on='store')\n",
    "\n",
    "# Previsões\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
    "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
    "\n",
    "# Reordenando\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.1 Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply(lambda x: np.sum(x), axis=0).reset_index().rename(columns={'index': 'Scenario', 0: 'Values'})\n",
    "df93['Values'] = df93['Values'].map('${:,.2f}'.format)\n",
    "df93"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.1 Machine Learning Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(22, 15);\n",
    "grid = GridSpec(3,2)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "sns.lineplot(x='date', y='sales', data=df9, label='SALES');\n",
    "sns.lineplot(x='date', y='predictions', data=df9, label='PREDICTIONS');\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "sns.lineplot(x='date', y='error_rate', data=df9);\n",
    "plt.axhline(1, linestyle='--')\n",
    "\n",
    "plt.subplot(grid[1,:])\n",
    "sns.scatterplot(data=df9, x='predictions', y='error');\n",
    "\n",
    "plt.subplot(grid[2,:])\n",
    "sns.kdeplot(df9['error'], fill=True, palette='magma');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.0 Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo treinado\n",
    "pickle.dump(model_xgb_tuned, open('model/model_rossmann.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.1 Rossman Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import inflection\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "class Rossmann(object):\n",
    "    def __init__(self):\n",
    "        self.home_path='/home/ricardo/repos/cds/6_ds_producao/'\n",
    "        self.competition_time_month_scaler = pickle.load(open(self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb'))\n",
    "        self.competition_distance_scaler   = pickle.load(open(self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb'))\n",
    "        self.promo_time_week_scaler        = pickle.load(open(self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb'))\n",
    "        self.year_scaler                   = pickle.load(open(self.home_path + 'parameter/year_scaler.pkl', 'rb'))\n",
    "        self.store_type_scaler             = pickle.load(open(self.home_path + 'parameter/store_type_scaler.pkl', 'rb'))\n",
    "\n",
    "    def data_cleaning(self, df1):\n",
    "        \n",
    "        # 1.1 Rename Columns\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo',\n",
    "                    'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "                    'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "                    'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "        cols_new = list(map(snakecase, cols_old))\n",
    "        df1.columns = cols_new\n",
    "        \n",
    "        # 1.3 Data Types\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "\n",
    "        #competition_distance -> filling the dataset with a high distance value\n",
    "        df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "        #competition_open_since_month\n",
    "        df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "        #competition_open_since_year\n",
    "        df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "        #promo2_since_week\n",
    "        df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "        #promo2_since_year\n",
    "        df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "        #promo_interval\n",
    "        month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "        df1['promo_interval'].fillna(0, inplace=True)\n",
    "        df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)\n",
    "\n",
    "        # 1.6 Change Types\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)\n",
    "\n",
    "        return df1\n",
    "    \n",
    "    def feature_engineering(self, df2):\n",
    "\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "        # competition since\n",
    "        df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year= x['competition_open_since_year'], month= x['competition_open_since_month'], day=1), axis=1)    \n",
    "        df2['competition_time_month'] = ((df2['date'] - df2['competition_since']) / 30).apply(lambda x: x.days).astype(int)\n",
    "                \n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "        df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w' ) - datetime.timedelta(days= 7))\n",
    "        df2['promo_time_week'] = ((df2['date'] - df2['promo_since']) / 7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        # assortment\n",
    "        df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "        # state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day')\n",
    "\n",
    "        # 3.1 Row Filtering\n",
    "        #df2 = df2[['open'] != 0]\n",
    "        \n",
    "        # 3.2 Column Filtering\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop(cols_drop, axis=1)\n",
    "\n",
    "        return df2\n",
    "    \n",
    "    def data_preparation (self, df5):\n",
    "\n",
    "        # Rescaling\n",
    "        # ajuste da coluna 'competition_distance' -> RobustScaler\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform(df5[['competition_distance']].values)\n",
    "        \n",
    "        # ajuste da coluna 'competition_time_month' -> RobustScaler\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform(df5[['competition_time_month']].values)\n",
    "        \n",
    "        # ajuste da coluna 'promo_time_week' -> MinMaxScaler\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform(df5[['promo_time_week']].values)\n",
    "        \n",
    "        # ajuste da coluna 'year' -> MinMaxScaler\n",
    "        df5['year'] = self.year_scaler.fit_transform(df5[['year']].values)\n",
    "        \n",
    "        # transformação dos conteúdos 'object' para 'int64' -> One Hot Encoding\n",
    "        df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "        # transformção dos conteúdos 'object' para 'int64' -> Label Encoding\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform(df5['store_type'])\n",
    "        \n",
    "        # transformação dos conteúdos 'object' para 'int64' -> Ordinal Encoding\n",
    "        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map(assortment_dict)\n",
    "\n",
    "        # 5.3.3 Nature Transformation\n",
    "        # ajuste dos períodos de tempo dos dias do mês, dias da semana, semanas e meses para valores cíclicos utilizando as fórmulas de seno e coseno\n",
    "        # colunas dos dias da semana\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * (2. * np.pi / 7)))\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * (2. * np.pi / 7)))\n",
    "\n",
    "        # dias do mês\n",
    "        df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * (2. * np.pi / 30)))\n",
    "        df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * (2. * np.pi / 30)))\n",
    "\n",
    "        # semanas\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * (2. * np.pi / 52)))\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * (2. * np.pi / 52)))\n",
    "\n",
    "        # meses\n",
    "        df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * (2. * np.pi / 12)))\n",
    "        df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * (2. * np.pi / 12)))\n",
    "\n",
    "        # Foi adicionada manualmente a coluna 'month_sin' que havia sido excluída pelo Boruta\n",
    "        cols_selected = ['store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "                         'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month',\n",
    "                         'promo_time_week', 'day_of_week_sin', 'day_of_week_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos',\n",
    "                         'week_of_year_sin', 'week_of_year_cos']\n",
    "\n",
    "        return df5[cols_selected]\n",
    "    \n",
    "    def get_prediction(self, model, original_data, test_data):\n",
    "        # Prediction\n",
    "        pred = model.predict(test_data)\n",
    "\n",
    "        # Joining pred into the original data\n",
    "        original_data['prediction'] = np.expm1(pred)\n",
    "\n",
    "        return original_data.to_json(orient='records', date_format='iso')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.2 API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask import Flask, Response, request\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "# Loading model\n",
    "model = pickle.load(open('model/model_rossman.pkl', 'rb'))\n",
    "\n",
    "# Initialize API\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('rossmann/predict', methods=['POST'])\n",
    "\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "\n",
    "    if test_json: # checking data\n",
    "        if isinstance(test_json, dict):\n",
    "            test_raw = pd.DataFrame(test_json, index=[0])\n",
    "\n",
    "        else: # multiple examples\n",
    "            test_raw = pd.DataFrame(test_json, columns=test_json[0].keys())\n",
    "\n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "\n",
    "        # Data cleaning\n",
    "        df1 = pipeline.data_cleaning(test_raw)\n",
    "\n",
    "        # Feature Engineering\n",
    "        df2 = pipeline.feature_engineering(df1)\n",
    "\n",
    "        # Data Preparation\n",
    "        df3 = pipeline.data_preparation(df2)\n",
    "\n",
    "        # Prediction\n",
    "        df_response = pipeline.get_prediction(model, test_raw, df3)\n",
    "\n",
    "        return df_response\n",
    "\n",
    "    else:\n",
    "        return Response('{}', status=200, mimetype='application/json')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.3 API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "df10 = pd.read_csv('csv_files/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging test + store dataset\n",
    "df_test = pd.merge(df10, df_store_raw, how='left', on='Store')\n",
    "\n",
    "# Choosing only one store for prediction test\n",
    "df_test = df_test[df_test['Store'] == 13]\n",
    "\n",
    "# Removing closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "df_test = df_test.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframe to json\n",
    "data = json.dumps(df_test.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Call\n",
    "#url = 'http://0.0.0.0:5000/rossmann/predict'\n",
    "#url = 'https://ricardoffdev-webapp-rossmann-handler-hyn9f6.streamlit.app/rossmann/predict'\n",
    "url = 'https://webapp-rossmann-q99w.onrender.com/rossmann/predict'\n",
    "\n",
    "header = {'Content-type': 'application/json'}\n",
    "data = data\n",
    "\n",
    "r = requests.post(url, data=data, headers=header)\n",
    "print('Status Code {}'.format(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(r.json(), columns=r.json()[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby('store').sum().reset_index()\n",
    "\n",
    "for x in range(len(d2)):\n",
    "    print('Store number {} will sell ${:,.2f} in the next 6 weeks'.format(\n",
    "           d2.loc[x, 'store'],\n",
    "           d2.loc[x, 'prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ide_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a399c2ac7b5b17bf514634d9aba7b4639a4982b0f452986d5f89cbb8104322d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
